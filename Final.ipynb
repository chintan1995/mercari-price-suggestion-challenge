{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-xmqEpPrhVB",
    "outputId": "823cbfa1-4747-4a4f-8046-51dd6ccec30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mEeEYd9yAvva"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Conv2D, MaxPool1D, MaxPool2D, AveragePooling1D, GlobalAveragePooling1D, \\\n",
    "                                    Flatten, Dropout, Dense, BatchNormalization, LayerNormalization, Concatenate\n",
    "from tensorflow.keras.layers import ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xc5Uaunz4F1E",
    "outputId": "0b8e6c75-1aaa-4d18-9c21-207d3d75ed79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import pickle\n",
    "import regex as re\n",
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/My Drive/Colab Notebooks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHhLgjeI_2UA"
   },
   "source": [
    "## **MLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XuxJYgA8X84"
   },
   "source": [
    "### **Feature Engineering for MLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QplH18PQMe0b"
   },
   "source": [
    "#### **Filling missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-2oH3_X_vzVK"
   },
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    df['name'].fillna('unk_name', inplace=True)\n",
    "    df['category_name'].fillna('unk_cat', inplace=True)\n",
    "    df['brand_name'].fillna('unk_brand', inplace=True)\n",
    "    df['item_description'].fillna('unk_descr', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5opYRiBm8ax4"
   },
   "source": [
    "#### **Text Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5cvBF5mK4U0p"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def decontract_text(phrase):\n",
    "    phrase = str(phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(sentence)\n",
    "    root = []\n",
    "    for w in words: \n",
    "        root.append(ps.stem(w))\n",
    "    return \" \".join(root)\n",
    "\n",
    "def preprocess_descriptive_text_column(sentance):\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    # we are removing the negative words from the stop words list: 'no', 'nor', 'not', 'shouldn't, won't, etc.\n",
    "    stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "                \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "                'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "                'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "                'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "                'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "                'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "                'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "                'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "                'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "                's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "                've', 'y']\n",
    "\n",
    "    sent = decontract_text(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "\n",
    "    root_sent = stem_sentence(sent.lower().strip())\n",
    "    return root_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvNDgAgr1MtL"
   },
   "source": [
    "#### **Fill Missing Brand Names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MC6xLHEQ8dqw"
   },
   "outputs": [],
   "source": [
    "def brand_guesser(df, existing_brands, brand_names_categories):\n",
    "    filled_brands = []\n",
    "    for row in df[['brand_name','name','category_name']].values:\n",
    "        found=False\n",
    "        if row[0]=='unk_brand':\n",
    "            for brand in existing_brands:\n",
    "                if brand in row[1].lower() and row[2] in brand_names_categories[brand]:\n",
    "                    filled_brands.append(brand)\n",
    "                    found=True\n",
    "                    break\n",
    "            if not found:\n",
    "                filled_brands.append('unk_brand')\n",
    "        else:\n",
    "            filled_brands.append(row[0])\n",
    "\n",
    "    df['brand_name']=filled_brands\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxhdGdRxLJ6n"
   },
   "source": [
    "#### **Split Categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "07gAsxd-2PPv"
   },
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    if text=='unk_cat':\n",
    "        return [\"No Label\", \"No Label\", \"No Label\"]\n",
    "    return text.split(\"/\")\n",
    "\n",
    "def split_categories(df):\n",
    "    df['general_cat'], df['subcat_1'], df['subcat_2'] = zip(*df['category_name'].apply(lambda x: split_text(x)))\n",
    "    df = df.drop('category_name', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHNNPNxGD4nr"
   },
   "source": [
    "#### **Adding Item Description len and Item Name len**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "F9mGn-fXHUHz"
   },
   "outputs": [],
   "source": [
    "def get_len_feature(col_series, scaler_text_len):\n",
    "    text_len = col_series.apply(lambda x: len(x.split()))\n",
    "    text_len = scaler_text_len.transform(text_len.values.reshape(-1, 1))\n",
    "    return text_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIYFv55Fy0jb"
   },
   "source": [
    "#### **Add is_expensive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Or-CEZZ6yzdN"
   },
   "outputs": [],
   "source": [
    "def get_is_expensive_feature(df, expensive_brands):\n",
    "    df['is_expensive'] = df['brand_name'].apply(lambda x: 1 if x in expensive_brands else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0LA9a3ZcTzG"
   },
   "source": [
    "#### **Make Shipping data sparse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nQZput5_cZHE"
   },
   "outputs": [],
   "source": [
    "def get_shipping_feature(df):\n",
    "    sparse_shipping = scipy.sparse.csr_matrix(df['shipping'].values)\n",
    "    sparse_shipping = sparse_shipping.reshape(-1,1) # Now the shape will be (1111901, 1)\n",
    "    return sparse_shipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpuksKS-_Th8"
   },
   "source": [
    "#### **Vectorizing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xOzmKqrzwq4_"
   },
   "outputs": [],
   "source": [
    "def vectorize_data(col_data, vectorizer):\n",
    "    ohe_data = vectorizer.transform(col_data)\n",
    "\n",
    "    return ohe_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPNhrvqEVjJ9"
   },
   "source": [
    "#### **Feature Engineering pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hgQ1y3ZrSATW"
   },
   "outputs": [],
   "source": [
    "def feature_pipeline_mlp(X_data, existing_brands, brand_names_categories, expensive_brands, general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, brand_name_vectorizer, item_name_vectorizer, \n",
    "                     item_desc_vectorizer, scaler_name_len, scaler_desc_len):\n",
    "    #print(\"Filling missing values...\")\n",
    "    X_data = fill_missing_values(X_data)\n",
    "\n",
    "    #print(\"pre-processing text data...\")\n",
    "    X_data['item_description'] = X_data['item_description'].apply(preprocess_descriptive_text_column)\n",
    "    X_data['name'] = X_data['name'].apply(preprocess_descriptive_text_column)\n",
    "    X_data['brand_name'] = X_data['brand_name'].apply(lambda x: str(x).lower())\n",
    "\n",
    "    #print(\"Guessing the missing brands...\")\n",
    "    X_data = brand_guesser(X_data, existing_brands, brand_names_categories)\n",
    "\n",
    "    #print(\"Splitting categories...\")\n",
    "    X_data = split_categories(X_data)\n",
    "\n",
    "    #print('Getting word lengths')\n",
    "    name_len =  get_len_feature(X_data['name'], scaler_name_len)\n",
    "    desc_len =  get_len_feature(X_data['item_description'], scaler_desc_len)\n",
    "\n",
    "    #print(\"Getting is_expensive brand feature...\")\n",
    "    sparse_is_expensive = get_is_expensive_feature(X_data, expensive_brands)\n",
    "\n",
    "    #print(\"Getting sparse shipping data...\")\n",
    "    sparse_shipping = get_shipping_feature(X_data)\n",
    "\n",
    "    #print(\"OHE vectorizing the text and categorical variables...\")\n",
    "    general_cat_ohe = vectorize_data(X_data['general_cat'].values.astype('U'), general_cat_vectorizer)\n",
    "    #print(\"general cat done...\")\n",
    "    subcat_1_ohe = vectorize_data(X_data['subcat_1'].values.astype('U'), subcat_1_vectorizer)\n",
    "    #print(\"sub cat 1 done...\")\n",
    "    subcat_2_ohe = vectorize_data(X_data['subcat_2'].values.astype('U'), subcat_2_vectorizer)\n",
    "    #print(\"sub cat 2 done...\")\n",
    "    brand_name_ohe = vectorize_data(X_data['brand_name'].values.astype('U'), brand_name_vectorizer)\n",
    "    #print(\"brand name done...\")\n",
    "    item_name_ohe = vectorize_data(X_data['name'], item_name_vectorizer)\n",
    "    #print(\"item name done...\")\n",
    "    item_desc_ohe = vectorize_data(X_data['item_description'], item_desc_vectorizer)\n",
    "    #print(\"item description done...\")\n",
    "\n",
    "    #print(\"Creating the final featurized dataset...\")\n",
    "    X_featurized = hstack((general_cat_ohe, subcat_1_ohe, subcat_2_ohe, brand_name_ohe, item_name_ohe, item_desc_ohe, \n",
    "                            desc_len, name_len, X_data['item_condition_id'].values.reshape(-1,1), sparse_shipping)).tocsr()\n",
    "\n",
    "    #print(\"Done!!!\\n---------------------------\\n\")\n",
    "    return X_featurized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bl78NSNpAbPp"
   },
   "source": [
    "## **CNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_85mkexjrUbR"
   },
   "source": [
    "### **Feature Engineering for CNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7OvsqZRrUbR"
   },
   "source": [
    "#### **Text Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FU82s7-wrUbR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def decontract_text(phrase):\n",
    "    phrase = str(phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def preprocess_descriptive_text_column_cnn(sentance):\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    # we are removing the negative words from the stop words list: 'no', 'nor', 'not', 'shouldn't, won't, etc.\n",
    "    stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "                \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "                'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "                'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "                'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "                'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "                'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "                'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "                'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "                'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "                's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "                've', 'y']\n",
    "\n",
    "    sent = decontract_text(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    sent = sent.lower()\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_TjH19SddlN"
   },
   "source": [
    "#### **Feature Engineering pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DfdFCuR4dccz"
   },
   "outputs": [],
   "source": [
    "def feature_pipeline_cnn(X_data, existing_brands, brand_names_categories, expensive_brands, scaler_name_len, scaler_desc_len):\n",
    "    #print(\"Filling missing values...\")\n",
    "    X_data = fill_missing_values(X_data)\n",
    "    \n",
    "    #print(\"pre-processing text data...\")\n",
    "    X_data['item_description'] = X_data['item_description'].apply(preprocess_descriptive_text_column_cnn) ### temp step ###\n",
    "    X_data['name'] = X_data['name'].apply(preprocess_descriptive_text_column)                         ### temp step ###\n",
    "    X_data['brand_name'] = X_data['brand_name'].apply(lambda x: str(x).lower())\n",
    "\n",
    "    #print(\"Guessing the missing brands...\")\n",
    "    X_data = brand_guesser(X_data, existing_brands, brand_names_categories)\n",
    "\n",
    "    #print(\"Splitting categories...\")\n",
    "    X_data = split_categories(X_data)\n",
    "\n",
    "    #print('Getting word lengths')\n",
    "    X_data['name_len'] =  get_len_feature(X_data['name'], scaler_name_len)\n",
    "    X_data['desc_len'] =  get_len_feature(X_data['item_description'], scaler_desc_len)\n",
    "\n",
    "    #print(\"Getting is_expensive brand feature...\")\n",
    "    X_data = get_is_expensive_feature(X_data, expensive_brands)\n",
    "\n",
    "    #print(\"Done!!!\\n---------------------------\\n\")\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSHET6QbC3_9"
   },
   "source": [
    "### **Get Input for CNNs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8E9fDV1AOt1p"
   },
   "outputs": [],
   "source": [
    "def categorical_embeddings(cat_data, le):\n",
    "    encoded_cat = le.transform(cat_data.values)\n",
    "\n",
    "    return encoded_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sDMIBfadrwp8"
   },
   "outputs": [],
   "source": [
    "def text_embeddings(text, tokenizer, max_len_doc):\n",
    "    # Word Tokenizer\n",
    "    encoded_docs_train = tokenizer.texts_to_sequences(text)\n",
    "    text_padded = pad_sequences(encoded_docs_train, maxlen=max_len_doc, padding='post')\n",
    "\n",
    "    return text_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ACR8rDsDIVxl"
   },
   "outputs": [],
   "source": [
    "def get_cnn_inputs(X_data, desc_tokenizer, name_tokenizer, desc_max_len_doc, name_max_len_doc, bn_le_ext, gc_le_ext, sc1_le_ext, sc2_le_ext):\n",
    "    ####################### Text data #######################\n",
    "    desc_text_padded = text_embeddings(X_data['item_description'].apply(str), desc_tokenizer, desc_max_len_doc)\n",
    "    name_text_padded = text_embeddings(X_data['name'].apply(str), name_tokenizer, name_max_len_doc)\n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ################### Categorical data ####################\n",
    "    bn_encoded = categorical_embeddings(X_data['brand_name'], bn_le_ext)\n",
    "    gc_encoded = categorical_embeddings(X_data['general_cat'], gc_le_ext)\n",
    "    sc1_encoded = categorical_embeddings(X_data['subcat_1'], sc1_le_ext)\n",
    "    sc2_encoded = categorical_embeddings(X_data['subcat_2'], sc2_le_ext)\n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    ################### Numeric data ########################\n",
    "    numeric_input = pd.concat((X_data['desc_len'], X_data['name_len']), axis=1).to_numpy()\n",
    "    #########################################################\n",
    "\n",
    "\n",
    "    X_cnn_inputs = [desc_text_padded, name_text_padded, bn_encoded, gc_encoded, sc1_encoded, sc2_encoded, tf.one_hot(X_data['item_condition_id'], 5),\\\n",
    "                    tf.one_hot(X_data['shipping'], 2), tf.one_hot(X_data['is_expensive'], 2), numeric_input]\n",
    "    return X_cnn_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUSuy_wWrg-G"
   },
   "source": [
    "# **INFERENCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0gKzSHV-afI"
   },
   "source": [
    "## **Importing pickles and models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zFrzh3MzS2LC"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This custom class is required to be loaded in production environment.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "class LabelEncoderExt(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "trz93Xfv-gVL"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/brands_list.pkl', 'rb') as f:\n",
    "    existing_brands = pickle.load(f)\n",
    "    \n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/brand_names_categories_dict.pkl', 'rb') as f:\n",
    "    brand_names_categories = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/expensive_brands.pkl', 'rb') as f:\n",
    "    expensive_brands = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/AAIC Submission/Pickled vars/vectorizers_scalers.pkl', 'rb') as f:\n",
    "    general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, \\\n",
    "                                                  brand_name_vectorizer, item_name_vectorizer, item_desc_vectorizer, \\\n",
    "                                                  scaler_name_len, scaler_desc_len = pickle.load(f)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/AAIC Submission/Pickled vars/desc_tokenizer.pkl', 'rb') as f:\n",
    "    desc_tokenizer = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/AAIC Submission/Pickled vars/name_tokenizer.pkl', 'rb') as f:\n",
    "    name_tokenizer = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/AAIC Submission/Pickled vars/desc_max_len_doc.pkl', 'rb') as f:\n",
    "    desc_max_len_doc = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/AAIC Submission/Pickled vars/name_max_len_doc.pkl', 'rb') as f:\n",
    "    name_max_len_doc = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/AAIC Submission/Pickled vars/bn_le_ext.pkl', 'rb') as f:\n",
    "    bn_le_ext = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/AAIC Submission/Pickled vars/gc_le_ext.pkl', 'rb') as f:\n",
    "    gc_le_ext = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/AAIC Submission/Pickled vars/sc1_le_ext.pkl', 'rb') as f:\n",
    "    sc1_le_ext = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/AAIC Submission/Pickled vars/sc2_le_ext.pkl', 'rb') as f:\n",
    "    sc2_le_ext = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3HD9uMnDUwXk"
   },
   "outputs": [],
   "source": [
    "mlp_model_path = \"/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/DL_Ensmeble/Model Checkpoints/Model_MLP_Checkpoints/BEST_MODEL.hdfs\"\n",
    "mlp_model_best = tf.keras.models.load_model(mlp_model_path)\n",
    "\n",
    "cnn_model_path = \"/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/DL_Ensmeble/Model Checkpoints/Model_CNN_Checkpoints/BEST_CNN_MODEL.hdfs\"\n",
    "cnn_model_best = tf.keras.models.load_model(cnn_model_path)\n",
    "\n",
    "ft_cnn_model_path = \"/content/drive/My Drive/Colab Notebooks/Applied AI Assignments/Case Study 1 Mercari Price Suggestion/zzFINAL COMPLETED/DL_Ensmeble/Model Checkpoints/Model_FT_CNN_Checkpoints/BEST_FT_CNN_MODEL.hdfs\"\n",
    "ft_cnn_model_best = tf.keras.models.load_model(ft_cnn_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qukCfRZ-f0l"
   },
   "source": [
    "## <font color='green'>**Final Inference Function**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWV5hSvj_kXh"
   },
   "outputs": [],
   "source": [
    "def final_inference(X):\n",
    "    raw_df = pd.DataFrame(X)\n",
    "    raw_df.columns = ['name', 'item_condition_id', 'category_name', 'brand_name', 'shipping', 'item_description']\n",
    "    X_data_mlp_inputs = feature_pipeline_mlp(raw_df, existing_brands, brand_names_categories, expensive_brands, \\\n",
    "                                         general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, \\\n",
    "                                         brand_name_vectorizer, item_name_vectorizer, item_desc_vectorizer, scaler_name_len, scaler_desc_len)\n",
    "    \n",
    "    X_data_cnn = feature_pipeline_cnn(raw_df, existing_brands, brand_names_categories, expensive_brands, scaler_name_len, scaler_desc_len)\n",
    "    X_cnn_inputs = get_cnn_inputs(X_data_cnn, desc_tokenizer, name_tokenizer, desc_max_len_doc, name_max_len_doc, bn_le_ext, gc_le_ext, sc1_le_ext, sc2_le_ext)\n",
    "\n",
    "    mlp_pred = mlp_model_best.predict(X_data_mlp_inputs)\n",
    "    ft_cnn_pred = ft_cnn_model_best.predict(X_cnn_inputs)\n",
    "    cnn_pred = cnn_model_best.predict(X_cnn_inputs)\n",
    "\n",
    "    mlp_pred = np.exp(mlp_pred)\n",
    "    ft_cnn_pred = np.exp(ft_cnn_pred)\n",
    "    cnn_pred = np.exp(cnn_pred)\n",
    "\n",
    "    ensemble_pred = (mlp_pred*0.5 + ft_cnn_pred*0.3 + cnn_pred*0.2)\n",
    "    return ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtKYlXyxKMUh",
    "outputId": "28076f7a-46ac-4e83-ccee-9849e16ce701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$8.152324\n",
      "CPU times: user 205 ms, sys: 3.01 ms, total: 208 ms\n",
      "Wall time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction = final_inference([[\"MLB Cincinnati Reds T Shirt Size XL\", 3, \"Men/Tops/T-shirts\", np.nan, 1, \"No description yet\"]])\n",
    "print(\"$\"+str(prediction[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw3LLts2mdPT"
   },
   "source": [
    "## <font color='green'>**Final Metric Function**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "-7zCbNzgmgyI"
   },
   "outputs": [],
   "source": [
    "def calculate_metric(y_act, y_pred):\n",
    "    rms = np.sqrt(mean_squared_log_error(y_act.values, y_pred))\n",
    "    return rms\n",
    "\n",
    "def final_metric(X, y):\n",
    "    X_data_mlp_inputs = feature_pipeline_mlp(X, existing_brands, brand_names_categories, expensive_brands, \\\n",
    "                                         general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, \\\n",
    "                                         brand_name_vectorizer, item_name_vectorizer, item_desc_vectorizer, scaler_name_len, scaler_desc_len)\n",
    "    \n",
    "    X_data_cnn = feature_pipeline_cnn(X, existing_brands, brand_names_categories, expensive_brands, scaler_name_len, scaler_desc_len)\n",
    "    X_cnn_inputs = get_cnn_inputs(X_data_cnn, desc_tokenizer, name_tokenizer, desc_max_len_doc, name_max_len_doc, bn_le_ext, gc_le_ext, sc1_le_ext, sc2_le_ext)\n",
    "\n",
    "    mlp_pred = mlp_model_best.predict(X_data_mlp_inputs)\n",
    "    ft_cnn_pred = ft_cnn_model_best.predict(X_cnn_inputs)\n",
    "    cnn_pred = cnn_model_best.predict(X_cnn_inputs)\n",
    "\n",
    "    mlp_pred = np.exp(mlp_pred)\n",
    "    ft_cnn_pred = np.exp(ft_cnn_pred)\n",
    "    cnn_pred = np.exp(cnn_pred)\n",
    "    ensemble_pred = (mlp_pred*0.5 + ft_cnn_pred*0.3 + cnn_pred*0.2)\n",
    "\n",
    "    metric = calculate_metric(y, ensemble_pred)\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train.tsv' not in os.listdir(): \n",
    "    !kaggle competitions download -c mercari-price-suggestion-challenge\n",
    "    get_ipython().system_raw(\"7z x \\*.7z && rm *.7z\")\n",
    "    get_ipython().system_raw(\"7z x \\*.zip && rm *.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXC8TXBkH-Uw",
    "outputId": "9ef7e908-8bba-4f28-dda9-355ccd67d6b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE metric = 0.3763031440856338\n"
     ]
    }
   ],
   "source": [
    "metric = final_metric(train_df.drop(['train_id','price'], axis=1), train_df['price'])\n",
    "print(\"RMSLE metric =\",metric)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OHhLgjeI_2UA",
    "3XuxJYgA8X84",
    "QplH18PQMe0b",
    "5opYRiBm8ax4",
    "AvNDgAgr1MtL",
    "sxhdGdRxLJ6n",
    "NHNNPNxGD4nr",
    "VIYFv55Fy0jb",
    "O0LA9a3ZcTzG",
    "xpuksKS-_Th8",
    "PPNhrvqEVjJ9",
    "Bl78NSNpAbPp",
    "_85mkexjrUbR",
    "z7OvsqZRrUbR",
    "a_TjH19SddlN",
    "zSHET6QbC3_9",
    "E0gKzSHV-afI"
   ],
   "machine_shape": "hm",
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
